{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37659310",
   "metadata": {},
   "source": [
    "Everything I learned about N-Grams here is from **Speech and Language Processing: An Introduction to Natural Language Processing Computational Linguistics, and Speech Recognition with Language Models** *Third Edition draft* from *Daniel Jurafsky*, and *James H. Martin*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fbfedc",
   "metadata": {},
   "source": [
    "# N-Grams Models\n",
    "So basically, when you try to predict what the next word is, you'd want to look back into the past of what you said earlier, and that could give you an estimate of what you should say next.  \n",
    "But how far back should you look? 1 word before? 2? IT'S N WORDS.  \n",
    "\n",
    "But it would be really hard to look back at all words, so we hold a ***Markov Assumption***. (some things won't be entirely clear why we hold them/use them/assume them which is a shortcoming from my end to not know yet, but we're learning.)\n",
    "\n",
    "The *Markov Assumption*:\n",
    "* So a Markov Property informs you that your future steps or decisions are memoryless, or basically independant from the past.\n",
    "    So here we will assume that we don't entirely depend on the past, or in other terms we won't look too far into the past.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d94d8bf",
   "metadata": {},
   "source": [
    "So given the sentence $s$: $<s>$ I love fries with $</s>$  \n",
    "* To find the probability of word $w$ given sentence $s$, assuming we use the full history:  \n",
    "$P(w|s) = P(w|I\\space love\\space fries\\space with) = \\frac{P(w \\cap s)}{P(s)} = \\frac{C(w \\cap s)}{C(s)} \\mid C(x) \\text{ is the count of x in our corpus.}$  \n",
    "* *You can figure out why the probability here would be equal to the count directly, I know such small details are mostly ignored assuming you would figure it out or that it's trivial but I wanted to give you a flasher.*  \n",
    "\n",
    "But writing and text are normally of variety and creative, which is why we dont't want to look at the full past each time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c5ec39",
   "metadata": {},
   "source": [
    "So the probability of the whole sentence $W$ = $\\{w_1, w_2, w_3...w_n\\}$ is:  \n",
    "$$P(w_1 w_2 w_3.....w_n) = P(w_1) P(w_2 | w_1) P(w_3 | w_1 w_2)...P(w_n | w_1 w_2 ... w_{n-1})  \\newline\n",
    "= P(w_{1:n}) = \\prod_{k = 1}^n{P(w_k | w_{1:k-1})}$$  \n",
    "\n",
    "But since we won't look at the full history, we could use the near history like the past word (*bi-gram*), the past two words (*tri-gram*), or maybe just the current word (*unigram*).  \n",
    "\n",
    "* In bigrams we predict our current word with the word before it:  \n",
    "$P(w_n|w_{1:n-1}) \\approx P(w_n|w_{n-1})$  \n",
    "\n",
    "So we assume our current word depends only on the previous word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b49a02",
   "metadata": {},
   "source": [
    "So the original equation we discussed:\n",
    "$$P(w_1 w_2 w_3.....w_n) = P(w_1) P(w_2 | w_1) P(w_3 | w_1 w_2)...P(w_n | w_1 w_2 ... w_{n-1})  \\newline\n",
    "= P(w_{1:n}) = \\prod_{k = 1}^n{P(w_k | w_{1:k-1})}$$  \n",
    "\n",
    "Should translate to bi-gram as follows:\n",
    "$$P(w_1 w_2 w_3.....w_n) = P(w_1) P(w_2 | w_1) P(w_3 | w_2)...P(w_n | w_{n-1})  \\newline\n",
    "= P(w_{1:n}) = \\prod_{k = 1}^n{P(w_k | w_{k-1})}$$  \n",
    "Where for each word we only calculate it's probability given the previous word only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e27179d",
   "metadata": {},
   "source": [
    "So if we try to estimate this probability using MLE (*Maximum Likelihood Estimation*).  \n",
    "So the probability of a word $w_n$ given it's previous word $w_{n-1}$ is the count of {$w_n w_{n-1}$} appearing in our corpus divided by the count of $w_{n-1}$ appearing in the corpus:\n",
    "* $P(w_n | w_{n-1}) = \\frac{C(w_{n-1}w_{n})}{w_{n-1}}$\n",
    "\n",
    "(in the reference they had an intermediate step of dividing by summation of the counts of $w_{n-1}$ with any other word $w$ which should be equal to just the count of the word $w_{n-1}$. But since for me this can follow intuitively from Bayes theorem for conditional probaility I just thought of going there directly)\n",
    "\n",
    "![He is Bayesian??](../images/Jojo.jpeg)  \n",
    "He is Bayesian??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ae7dcb",
   "metadata": {},
   "source": [
    "So we can generalize what we have so far to $N$ grams now, so that we $N$ tokens into the past to predict the next word.  \n",
    "- $P(w_n | w_{n-N+1 : n-1}) = \\frac{C(w_{n-N+1 : n-1} w_n)}{C(w_{n-N+1 : n-1})}$  \n",
    "\n",
    "So the whole sentence $W$ probability:  \n",
    "- $P(W) = \\prod_{k=1}^n{P(w_k | w_{k-N+1 : k-1})}$  \n",
    "\n",
    "So $N = 1$ would be unigram, $N = 2$ is bigram and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4af2c2",
   "metadata": {},
   "source": [
    "Since the goal of this small journey is to learn about embeddings I won't push further with n-grams, so lets try to make a simple bigram model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50c48ed",
   "metadata": {},
   "source": [
    "I wanted to use any random text dataset, so I found this Twitter hate speech dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "cc6c3c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "splits = {\"train\": \"training set.csv\", \"test\": \"testing set.csv\"}\n",
    "df = pd.read_csv(\"hf://datasets/thefrankhsu/hate_speech_twitter/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f421bdcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tweet",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "42934909-e664-4146-89bd-c538948d25f8",
       "rows": [
        [
         "0",
         "krazy i dont always get drunk and pass out but when i do when did they start making colored duct tape"
        ],
        [
         "1",
         "white kids favorite activities calling people niggers on xboxfucking their pet shooting up their school"
        ],
        [
         "2",
         "maam did you clear that tweet with the   careful they may brand you race traitor for the nerve of thinking"
        ],
        [
         "3",
         "wth is that playing missy  i mean seriously rt republicann this movie gone be trash"
        ],
        [
         "4",
         "he promised to stand with the muzzies so"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>krazy i dont always get drunk and pass out but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white kids favorite activities calling people ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>maam did you clear that tweet with the   caref...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wth is that playing missy  i mean seriously rt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>he promised to stand with the muzzies so</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0  krazy i dont always get drunk and pass out but...\n",
       "1  white kids favorite activities calling people ...\n",
       "2  maam did you clear that tweet with the   caref...\n",
       "3  wth is that playing missy  i mean seriously rt...\n",
       "4           he promised to stand with the muzzies so"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   lets just drop label and category columns since we won't need them\n",
    "df.drop(labels=[\"label\", \"categories\"], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deab47a",
   "metadata": {},
   "source": [
    "So now we could go multiple ways, I am not aware of all the implementations of N-Grams.  \n",
    "But one basic way of thinking is saying I will make a dictionary where each word is associated with an index for a lookup table, or maybe a hash function.  \n",
    "Then, make the lookup table were each row-column pair refer to a specific bigram probability, so row 0 col 1 refers to *the probability of row 0's word coming after col 0's word*  \n",
    "We could then optimize the the space and time usage by just keeping the max probability of each column which will be always chosen anyway.\n",
    "I am sure there are many optimizations but we just want to apply the theorem we have.  \n",
    "\n",
    "One approach I saw is making a dictionary of each word available, were the values of each word key will be a list of all other words that appeared directly after it in any sentence. Maybe we could sort each list's values and get the word with highest frequency. **I will use this approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "395828cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5679, 1)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5936ef7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'after deztinis session we needed this lmao jk girl good yob lol'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tweet\"][50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "45420670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5679,)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#   lets decompose each sentence into a list of words\n",
    "sentences = df.to_numpy()\n",
    "sentences = sentences.flatten()\n",
    "sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a6310e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [str(sentence).split(\" \") for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e62f6957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['are',\n",
       " 'are',\n",
       " 'go',\n",
       " 'guess',\n",
       " 'go',\n",
       " 'who',\n",
       " 'weak',\n",
       " 'are',\n",
       " 'dream',\n",
       " 'are',\n",
       " 'claim',\n",
       " 'flipping',\n",
       " 'should',\n",
       " 'would',\n",
       " 'should',\n",
       " 'should',\n",
       " 'changethename']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I think since this dataset is of twitter, the vocabulary we has will be greatly inconsistent as spelling, etc.\n",
    "# It could be benefecial to keep the capitalization of the words which would indicate the place it's used in text better\n",
    "# but since these are tweets then I wouldn't count much on it and will just make all lowercase.\n",
    "\n",
    "bigram = {}\n",
    "for sentence in sentences:\n",
    "    for index, word in enumerate(sentence[:-1]):\n",
    "        if (\n",
    "            word.isalnum()\n",
    "        ):  # I don't think things other than alphanumerals would be needed\n",
    "            word = word.lower()\n",
    "            next_word = sentence[index + 1].lower()\n",
    "\n",
    "            if word in bigram.keys():\n",
    "                bigram[word].append(next_word)\n",
    "            else:\n",
    "                bigram[word] = []\n",
    "                bigram[word].append(next_word)\n",
    "bigram[\"americans\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8ec54e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Now let's get the frequency of each word given it's previous word (the key)\n",
    "#   So what happens here is we reconstruct the the bigram dictionary\n",
    "#   where the list of next words for each word will be reduced into a set of each next word and it's frequency\n",
    "#   (but like (frequency, next_word) tuples so the set sorts itself on the frequency) ;)\n",
    "\n",
    "bigram = {\n",
    "    word: set((next_words.count(next_word), next_word) for next_word in next_words)\n",
    "    for word, next_words in bigram.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "9b4aca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think we can just take the next word with highest frequency for each word and we will be good to go\n",
    "bigram = {word: list(next_words)[-1][1] for word, next_words in bigram.items()}   # get the next_word in last (frequency, next_word) tuple in the set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063b3b76",
   "metadata": {},
   "source": [
    "So there you have it, the bigram dictionary. We can make a simple sentence generator now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "eb725701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_sentence(first_word: str | None = None, length: int = 10):\n",
    "    sentence = []\n",
    "    if first_word:\n",
    "        sentence.append(first_word)\n",
    "        next_word = bigram[first_word]\n",
    "        for _ in range(length):\n",
    "            sentence.append(next_word)\n",
    "            next_word = bigram[next_word] if next_word in bigram.keys() else \".\"\n",
    "            if next_word == \".\":\n",
    "                break\n",
    "    else:\n",
    "        vocab = list(bigram.keys())\n",
    "        vocab_length = len(vocab)\n",
    "        \n",
    "        first_word = bigram[vocab[random.randint(0, vocab_length-1)]]\n",
    "        \n",
    "        sentence.append(first_word)\n",
    "        next_word = bigram[first_word]\n",
    "        for _ in range(length):\n",
    "            sentence.append(next_word)\n",
    "            next_word = bigram[next_word] if next_word in bigram.keys() else \".\"\n",
    "            if next_word == \".\":\n",
    "                break\n",
    "    return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f5c23d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello dare place mostly just watching barney'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "34fbc444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spongebob and diplomatic on everything about a guinea fowls are you'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0461388d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trash thursday capped a guinea fowls are you felt all over'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a4b09b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lea lahi mai'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ff2a0374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in zebra stripey'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5afe936e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with marinated chickenshrimp my guinea fowls are you felt all over'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916c8bda",
   "metadata": {},
   "source": [
    "This is just a run down of what I learned, I will probably learn more and also probably, as simple as this notebook can get, find gaps or issues with the explanation or procedures in this notebook, so it's probably okay if you find some too, point it out and help us both learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
