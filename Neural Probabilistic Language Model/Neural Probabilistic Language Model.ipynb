{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cd0084f",
   "metadata": {},
   "source": [
    "The explanations and what I will apply is from and after reading ***A neural probabilistic language model paper*** *(Bengio, Y., et al, 2003)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc880eec",
   "metadata": {},
   "source": [
    "So we learned about n-grams language models, and how we can predict the next word given the $n$ previous words. And we implemented a **very simple** bigram model.  \n",
    "Now, my next step was to understand word embeddings is how n-grams and statistical language modeling translated into neural networks and deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc566e",
   "metadata": {},
   "source": [
    "So in *(Bengio, Y., et al, 2003)*, the motivations they had is that it's difficult to model the joint probablity distribution of multiple consecutive words (random variables with **discrete** values.) An example would be having a sentence of 10 words where each one could have any value of a vocabulary of $100000$ words, so you have about $100000^{10}$ different possibilities or as they say \"free parameters\" for that sentence.\n",
    "\n",
    "They also explained that modeling continuous variables is easier for generalization because the approximation function we want to learn is expected to **have** some smooth properties (neural networks would be a good example.) So the continuous space has a smoother space unlike discrete spaces where are more, idk, **discrete?** And they explain more about discrete spaces:  \n",
    "***\"For discrete spaces, the generalization structure is not as obvious: any change of these discrete variables may have a drastic impact on the value of the function to be estimated, and when the number of values that each discrete variable can take is large, most observed objects are almost maximally far from each other in hamming distance.\"***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e064b2b0",
   "metadata": {},
   "source": [
    "**<u>Disclaimer:</u>**  \n",
    "We are now going into the approach proposed and the mathematics and the design of it. Since I myself am trying to learn here, I won't be following the paper exactly here, meaning that I could:  \n",
    "1.  Add a little bit more mathematics to give you and myself a better idea of what's happening.  \n",
    "2.  Branch quickly in what I am writing to explain some other concepts that will help us understand what we have here better, which might be trivial to you depending on your level and knowledge so you can skip. But it won't be long anyway, this isn't an article (yet!)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef7199b",
   "metadata": {},
   "source": [
    "So previously, we learned that you can predict the next word $w_t$ given all previous ones:  \n",
    "    $$P(w_t | w_{1:t-1}) \\tag{1}$$  \n",
    "    \n",
    "But in n-grams, we wanted to approximate $(1)$ by using just the $n$ words before $w_t$ as follows:  \n",
    "    $$P(w_t | w_{t-n+1:t-1}) \\approx P(w_t | w_{1:t-1}) \\tag{2}$$\n",
    "\n",
    "Remember the joint probability equation, assuming there's a dependance between variables $x$, $y$, and $z$ (which in our case are words):\n",
    "    $$P(x \\cap y \\cap z) = P(x, y, z) = P(z) P(y | z) P(x | z, y) \\tag{3}$$\n",
    "Which can be generalized to:\n",
    "    $$P(x_i, x_{i-1}...x_{1}) = \\prod_{k = 1}^n{P(x_k | x_1, x_2...x_{k-1})} \\tag{4}$$\n",
    "\n",
    "And now that we can approximate the probability of $w_t$, we can get the joint probability of a sentence or a sequence of words $W$ of length $T$:  \n",
    "\n",
    "$$P(w_t w_{t-1}...w_1) = \\prod_{k=1}^T{P(w_k | w_{k-n+1:k-1})} \\tag{5}$$\n",
    "\n",
    "This is the function we would want to maximize.\n",
    "\n",
    "Side note for the joint probability in $(3)$: if we assumed there's no dependance between $x$, $y$, and $z$, we have:\n",
    "    $$P(x, y, z) = P(x) P(y) P(z) \\tag{6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3fe590",
   "metadata": {},
   "source": [
    "We are now want to use neural network to find a good function $f$ that approximates this joint probability distribution for a specific corpus. $f$ would have a set of trainable parameters $\\Theta$.  \n",
    "Now from above and $(2)$:  \n",
    "$$P(w_t | w_{t-n+1:t-1}) \\approx P(w_t | w_{1:t-1}) \\newline\n",
    "\\approx f(w_t, w_{t-1}...w_{t-n+1};\\Theta) \\tag{7}$$\n",
    "\n",
    "You can notice that $f$ uses $w_t$ and the n previous words $(w_{t-1}...w{t-n+1})$, just like we do in n-grams modelling.  \n",
    "Now, from $(5)$ & $(7)$, we want to get the joint probability of the whole word sequence $W$ of word count $T$:\n",
    "$$P(w_t w_{t-1}...w_1) = \\prod_{k=1}^T{P(w_k | w_{k-n+1:k-1})} \\newline\n",
    "\\approx \\prod_{t = 1}^T{f(w_t, w_{t-1}...w_{t-n+1};\\Theta)} \\tag{8}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca14ee96",
   "metadata": {},
   "source": [
    "<u>What are the parameters included in $\\Theta$?</u>\n",
    "\n",
    "The parameters we have would depend on the model's architecture, we have mainly 2 parts (according to the architecture proposed in the text.):\n",
    "1. Assuming we would have a set of vocabulary $V$ of size $|V|$, we want to create a mapping (a lookup table) $C$ for each word $w_i$ to a vector $v_{w_i}$ whose dimensions $\\in \\mathbb{R}^m$. In other words: $C(w_i) = v_{w_i} \\mid v_{w_i} \\in \\mathbb{R}^m$.\n",
    "2. The probability function $g$ that takes the mapping $C$ of the input words and maps them to the probability distribution over the vocab $V$.\n",
    "    $$f(w_t, w_{t-1}...w_{t-n+1};\\Theta) = g(i, C(w_{t-1}...w_{t-n+1}))$$\n",
    "\n",
    "So $g$ outputs a vector of $|V|$ dimensions ($\\in \\mathbb{R}^{|V|}$), where the $i_{th}$ dimension or element corresponds to the probability of $i_{th}$ word being the next token:\n",
    "$$\\hat{P}(w_t=i | w_{1:t-1})$$\n",
    "\n",
    "Then $C$ would be an $|V| \\times m$ matrix. You can also say that $C$ is the word embeddings as a term we know of today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f75abbc",
   "metadata": {},
   "source": [
    "We now want to start definining the loss function, which will be equivalent to $(8)$:\n",
    "$$l = \\prod_{t = 1}^T{f(w_t, w_{t-1}...w_{t-n+1};\\Theta)} \\tag{9}$$\n",
    "\n",
    "We can transform the product into a summation by taking the log of both sides:\n",
    "$$log(l) = log(\\prod_{t = 1}^T{f(w_t, w_{t-1}...w_{t-n+1};\\Theta)}) \\newline\n",
    "= \\sum_{t = 1}^T{log(f(w_t, w_{t-1}...w_{t-n+1};\\Theta))} \\tag{10}$$\n",
    "**<u>Why do we take the log:</u>**  \n",
    "* Multiplying a lot of probabilities would make the resulting value approach $0$, or a numerical underflow when running on a computer which can mess up the calculations and results.\n",
    "* Likewise if we were multiplying large numbers would result in an even bigger result, eventually leading to an overflow.\n",
    "\n",
    "What we reached in $(10)$ is what we call Log Likelihood. So, from $(10)$, our loss function would be:\n",
    "$$L = \\sum_{t = 1}^T{log(f(w_t, w_{t-1}...w_{t-n+1};\\Theta))} \\tag{11}$$\n",
    "\n",
    "We can also add a Regularization function $R$ that regularizes the set of parameters $\\Theta$, or a subset of $\\Theta$ called $\\Omega$:\n",
    "$$L = \\sum_{t = 1}^T{log(f(w_t, w_{t-1}...w_{t-n+1};\\Theta))} + R(\\Omega) \\tag{12}$$\n",
    "\n",
    "But I won't use the regularized term $R(\\Omega)$ in this implementation, so we will use $(11)$ as our Loss Function for the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4a48a4",
   "metadata": {},
   "source": [
    "Now for the network's architecture:  \n",
    "<p align=\"center\">\n",
    "    <img src=\"../images/model_architecture.jpeg\" alt=\"Bengio, et al. 2003\" width=\"600\"/>\n",
    "</p>  \n",
    "\n",
    "The first layer is the embeddings layer, then we have just one hidden layer before the $Softmax$ function. They also describe that **we can optionally connect the embeddings layer directly to the softmax layer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440b1c00",
   "metadata": {},
   "source": [
    "**<u>What what are the parameters we have?</u>**  \n",
    "\n",
    "$$\\Theta = (b, d, W, U, H, C)$$\n",
    "\n",
    "| Parameter | Size | Optional | Description |\n",
    "|:----------:|:----------:|:----------:|:----------:|\n",
    "| $C$    | $\\|V\\| \\times m$   | No   | The lookup table for the vector representation of each word in the vocabulary $\\|V\\|$.   |\n",
    "| $H$    | $h \\times (n-1) m$  | No   | The weights matrix for the hidden layer.   |\n",
    "| $U$    | $\\|V\\| \\times h$  | No   | The weights matrix for the hidden layer output to the softmax layer.   |\n",
    "| $b$    | $\\|V\\|$  | No   | The output layer's biases.   |\n",
    "| $d$    | $h$  | No   | The hidden layer's biases.   |\n",
    "| $W$    | $\\|V\\| \\times (n-1)m$  | Yes   | The weights matrix for optional connection between the embeddings layer and softmax layer.   |\n",
    "\n",
    "Such that the logits of the output layer y that's passed to the softmax activation:\n",
    "$$y = b + Wx + U tanh(d + Hx)$$\n",
    "\n",
    "The softmax activation (predicted probability of word $w_t$ given the previous $n$):\n",
    "$$\\hat{P}(w_t | w_{t-1:t-n+1}) = \\frac{\\textit{e}^{y_{w_t}}}{\\sum_{i}{\\textit{e}^{y_i}}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df072bcd",
   "metadata": {},
   "source": [
    "Perhaps that's enough reading, lets try to make a naive implementation of the architecture above with ***PyTorch***.  \n",
    "And by ***naive*** I mean it's my first time building a Torch module that deals with sequences, so I probably won't do it the most efficient way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4418fd42",
   "metadata": {},
   "source": [
    "# Neural Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5111610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# Our Neural Language Model class\n",
    "class NLM(nn.Module):\n",
    "    def __init__(self, v: int, m: int, n: int, h: int):\n",
    "        super().__init__()\n",
    "        self.v: int = v  #   Vocabulary size\n",
    "        self.m: int = m  #   Embeddings dimensions\n",
    "        self.n: int = n  #   Context size (the n-grams)\n",
    "        self.h: int = h  #   Hidden layer units (neurons)\n",
    "\n",
    "        # I started with a randn tensor of shape (v, m) but turns out that Torch has a class for embeddings specifically,\n",
    "        # where this class stores the lookup table with numerical indices, so we still have to map the words to indices before entering the class\n",
    "        # or tokenization in other means\n",
    "        self.c = nn.Embedding(\n",
    "            num_embeddings=v,\n",
    "            embedding_dim=m,\n",
    "        )\n",
    "\n",
    "        # This way, the linear/Activation layer will take flattened embeddings and produce v outputs\n",
    "        self.linear1 = nn.Linear(in_features=m * (n - 1), out_features=h)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # The output layer, linear to create a logits and softmax activation to normalize them to probabilities\n",
    "        self.linear2 = nn.Linear(in_features=h, out_features=v)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.c(x)\n",
    "\n",
    "        linear_activation = self.linear1(embeddings)\n",
    "        non_linear_activation = self.tanh(linear_activation)\n",
    "\n",
    "        logits = self.linear2(non_linear_activation)\n",
    "        probabilities = self.softmax(logits)\n",
    "\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865821e6",
   "metadata": {},
   "source": [
    "# Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4b9d40",
   "metadata": {},
   "source": [
    "I will try to stay close to the paper as much as I can, so we will use brown corpus from nltk, which should be the same they use in the paper, or at least **similar**.  \n",
    "I don't want to drift off so we can compere the results we get to the ones in the paper, but any variations that I do from the one in the paper would probably mean different numbers which means I could've either drifted too much or implemented something incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2251b8cd",
   "metadata": {},
   "source": [
    "## Understand the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6705cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/shadyali/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<CategorizedTaggedCorpusReader in '/Users/shadyali/nltk_data/corpora/brown'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe290c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1915fdf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd86bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56057"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(brown.words()))     # unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "72200864",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_words= [word for word in set(brown.words()) if not word.isalnum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6c3d5f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " 'sea-food',\n",
       " 'husband-wife',\n",
       " 'herd-owner',\n",
       " \"Skolovsky's\",\n",
       " '7.6%',\n",
       " \"Vonnegut's\",\n",
       " 'working-class',\n",
       " '32-degrees-F',\n",
       " '400-401',\n",
       " 'south-central',\n",
       " 'pistol-packing',\n",
       " 'merry-go-round',\n",
       " 'wine-',\n",
       " \"'55\"]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_index = non_words.index(\".\")\n",
    "non_words[dot_index:dot_index+15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0356259",
   "metadata": {},
   "source": [
    "So we have about $56k$ unique words, which includes punctuations, words mixed with punctuations, alphanumerals, etc..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2e67ca86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.tagged_sents()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
