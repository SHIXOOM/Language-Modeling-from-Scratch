{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cd0084f",
   "metadata": {},
   "source": [
    "The explanations and what I will apply is from and after reading ***A neural probabilistic language model paper*** *(Bengio, Y., et al, 2003)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc880eec",
   "metadata": {},
   "source": [
    "So we learned about n-grams language models, and how we can predict the next word given the $n$ previous words. And we implemented a **very simple** bigram model.  \n",
    "Now, my next step was to understand word embeddings is how n-grams and statistical language modeling translated into neural networks and deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc566e",
   "metadata": {},
   "source": [
    "So in *(Bengio, Y., et al, 2003)*, the motivations they had is that it's difficult to model the joint probablity distribution of multiple consecutive words (random variables with **discrete** values.) An example would be having a sentence of 10 words where each one could have any value of a vocabulary of $100000$ words, so you have about $100000^{10}$ different possibilities or as they say \"free parameters\" for that sentence.\n",
    "\n",
    "They also explained that modeling continuous variables is easier for generalization because the approximation function we want to learn is expected to **have** some smooth properties (neural networks would be a good example.) So the continuous space has a smoother space unlike discrete spaces where are more, idk, **discrete?** And they explain more about discrete spaces:  \n",
    "***\"For discrete spaces, the generalization structure is not as obvious: any change of these discrete variables may have a drastic impact on the value of the function to be estimated, and when the number of values that each discrete variable can take is large, most observed objects are almost maximally far from each other in hamming distance.\"***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e064b2b0",
   "metadata": {},
   "source": [
    "**<u>Disclaimer:</u>**  \n",
    "We are now going into the approach proposed and the mathematics and the design of it. Since I myself am trying to learn here, I won't be following the paper exactly here, meaning that I could:  \n",
    "1.  Add a little bit more mathematics to give you and myself a better idea of what's happening.  \n",
    "2.  Branch quickly in what I am writing to explain some other concepts that will help us understand what we have here better, which might be trivial to you depending on your level and knowledge so you can skip. But it won't be long anyway, this isn't an article (yet!)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef7199b",
   "metadata": {},
   "source": [
    "So previously, we learned that you can predict the next word $w_t$ given all previous ones:  \n",
    "    $$P(w_t | w_{1:t-1}) \\tag{1}$$  \n",
    "    \n",
    "But in n-grams, we wanted to approximate $(1)$ by using just the $n$ words before $w_t$ as follows:  \n",
    "    $$P(w_t | w_{t-n+1:t-1}) \\approx P(w_t | w_{1:t-1}) \\tag{2}$$\n",
    "\n",
    "Remember the joint probability equation, assuming there's a dependance between variables $x$, $y$, and $z$ (which in our case are words):\n",
    "    $$P(x \\cap y \\cap z) = P(x, y, z) = P(z) P(y | z) P(x | z, y) \\tag{3}$$\n",
    "Which can be generalized to:\n",
    "    $$P(x_i, x_{i-1}...x_{1}) = \\prod_{k = 1}^n{P(x_k | x_1, x_2...x_{k-1})} \\tag{4}$$\n",
    "\n",
    "And now that we can approximate the probability of $w_t$, we can get the joint probability of a sentence or a sequence of words $W$ of length $T$:  \n",
    "\n",
    "$$P(w_t w_{t-1}...w_1) = \\prod_{k=1}^T{P(w_k | w_{k-n+1:k-1})} \\tag{5}$$\n",
    "\n",
    "This is the function we would want to maximize.\n",
    "\n",
    "Side note for the joint probability in $(3)$: if we assumed there's no dependance between $x$, $y$, and $z$, we have:\n",
    "    $$P(x, y, z) = P(x) P(y) P(z) \\tag{6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3fe590",
   "metadata": {},
   "source": [
    "We are now want to use neural network to find a good function $f$ that approximates this joint probability distribution for a specific corpus. $f$ would have a set of trainable parameters $\\Theta$.  \n",
    "Now from above and $(2)$:  \n",
    "$$P(w_t | w_{t-n+1:t-1}) \\approx P(w_t | w_{1:t-1}) \\newline\n",
    "\\approx f(w_t, w_{t-1}...w_{t-n+1};\\Theta) \\tag{7}$$\n",
    "\n",
    "You can notice that $f$ uses $w_t$ and the n previous words $(w_{t-1}...w{t-n+1})$, just like we do in n-grams modelling.  \n",
    "Now, from $(5)$ & $(7)$, we want to get the joint probability of the whole word sequence $W$ of word count $T$:\n",
    "$$P(w_t w_{t-1}...w_1) = \\prod_{k=1}^T{P(w_k | w_{k-n+1:k-1})} \\newline\n",
    "\\approx \\prod_{t = 1}^T{f(w_t, w_{t-1}...w_{t-n+1};\\Theta)} \\tag{8}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca14ee96",
   "metadata": {},
   "source": [
    "<u>What are the parameters included in $\\Theta$?</u>\n",
    "\n",
    "The parameters we have would depend on the model's architecture, we have mainly 2 parts (according to the architecture proposed in the text.):\n",
    "1. Assuming we would have a set of vocabulary $V$ of size $|V|$, we want to create a mapping (a lookup table) $C$ for each word $w_i$ to a vector $v_{w_i}$ whose dimensions $\\in \\mathbb{R}^m$. In other words: $C(w_i) = v_{w_i} \\mid v_{w_i} \\in \\mathbb{R}^m$.\n",
    "2. The probability function $g$ that takes the mapping $C$ of the input words and maps them to the probability distribution over the vocab $V$.\n",
    "    $$f(w_t, w_{t-1}...w_{t-n+1};\\Theta) = g(i, C(w_{t-1}...w_{t-n+1}))$$\n",
    "\n",
    "So $g$ outputs a vector of $|V|$ dimensions ($\\in \\mathbb{R}^{|V|}$), where the $i_{th}$ dimension or element corresponds to the probability of $i_{th}$ word being the next token:\n",
    "$$\\hat{P}(w_t=i | w_{1:t-1})$$\n",
    "\n",
    "Then $C$ would be an $|V| \\times m$ matrix. You can also say that $C$ is the word embeddings as a term we know of today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f75abbc",
   "metadata": {},
   "source": [
    "We now want to start definining the loss function, which will be equivalent to $(8)$:\n",
    "$$l = \\prod_{t = 1}^T{f(w_t, w_{t-1}...w_{t-n+1};\\Theta)} \\tag{9}$$\n",
    "\n",
    "We can transform the product into a summation by taking the log of both sides:\n",
    "$$log(l) = log(\\prod_{t = 1}^T{f(w_t, w_{t-1}...w_{t-n+1};\\Theta)}) \\newline\n",
    "= \\sum_{t = 1}^T{log(f(w_t, w_{t-1}...w_{t-n+1};\\Theta))} \\tag{10}$$\n",
    "**<u>Why do we take the log:</u>**  \n",
    "* Multiplying a lot of probabilities would make the resulting value approach $0$, or a numerical underflow when running on a computer which can mess up the calculations and results.\n",
    "* Likewise if we were multiplying large numbers would result in an even bigger result, eventually leading to an overflow.\n",
    "\n",
    "What we reached in $(10)$ is what we call Log Likelihood. So, from $(10)$, our loss function would be:\n",
    "$$L = \\sum_{t = 1}^T{log(f(w_t, w_{t-1}...w_{t-n+1};\\Theta))} \\tag{11}$$\n",
    "\n",
    "We can also add a Regularization function $R$ that regularizes the set of parameters $\\Theta$, or a subset of $\\Theta$ called $\\Omega$:\n",
    "$$L = \\sum_{t = 1}^T{log(f(w_t, w_{t-1}...w_{t-n+1};\\Theta))} + R(\\Omega) \\tag{12}$$\n",
    "\n",
    "But I won't use the regularized term $R(\\Omega)$ in this implementation, so we will use $(11)$ as our Loss Function for the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4a48a4",
   "metadata": {},
   "source": [
    "Now for the network's architecture:  \n",
    "<p align=\"center\">\n",
    "    <img src=\"../images/model_architecture.jpeg\" alt=\"Bengio, et al. 2003\" width=\"600\"/>\n",
    "</p>  \n",
    "\n",
    "The first layer is the embeddings layer, then we have just one hidden layer before the $Softmax$ function. They also describe that **we can optionally connect the embeddings layer directly to the softmax layer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440b1c00",
   "metadata": {},
   "source": [
    "**<u>What what are the parameters we have?</u>**  \n",
    "\n",
    "$$\\Theta = (b, d, W, U, H, C)$$\n",
    "\n",
    "| Parameter | Size | Optional | Description |\n",
    "|:----------:|:----------:|:----------:|:----------:|\n",
    "| $C$    | $\\|V\\| \\times m$   | No   | The lookup table for the vector representation of each word in the vocabulary $\\|V\\|$.   |\n",
    "| $H$    | $h \\times (n-1) m$  | No   | The weights matrix for the hidden layer.   |\n",
    "| $U$    | $\\|V\\| \\times h$  | No   | The weights matrix for the hidden layer output to the softmax layer.   |\n",
    "| $b$    | $\\|V\\|$  | No   | The output layer's biases.   |\n",
    "| $d$    | $h$  | No   | The hidden layer's biases.   |\n",
    "| $W$    | $\\|V\\| \\times (n-1)m$  | Yes   | The weights matrix for optional connection between the embeddings layer and softmax layer.   |\n",
    "\n",
    "Such that the logits of the output layer y that's passed to the softmax activation:\n",
    "$$y = b + Wx + U tanh(d + Hx)$$\n",
    "\n",
    "The softmax activation (predicted probability of word $w_t$ given the previous $n$):\n",
    "$$\\hat{P}(w_t | w_{t-1:t-n+1}) = \\frac{\\textit{e}^{y_{w_t}}}{\\sum_{i}{\\textit{e}^{y_i}}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df072bcd",
   "metadata": {},
   "source": [
    "Perhaps that's enough reading, lets try to make a naive implementation of the architecture above with ***PyTorch***.  \n",
    "And by ***naive*** I mean it's my first time building a Torch module that deals with sequences, so I probably won't do it the most efficient way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4418fd42",
   "metadata": {},
   "source": [
    "# Neural Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "5111610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "# Our Neural Language Model class\n",
    "class NLM(nn.Module):\n",
    "    def __init__(self, v: int, m: int, n: int, h: int):\n",
    "        super().__init__()\n",
    "        self.v: int = v  #   Vocabulary size\n",
    "        self.m: int = m  #   Embeddings dimensions\n",
    "        self.n: int = n  #   Context size (the n-grams), we will actually input n-1 words to predict the n^th\n",
    "        self.h: int = h  #   Hidden layer units (neurons)\n",
    "        self.padding_token_idx: int = v  #   Indicates the index of the special token to be used to pad the a sequence of length < the context size\n",
    "\n",
    "        #   I started with a randn tensor of shape (v, m) but turns out that Torch has a class for embeddings specifically,\n",
    "        #   where this class stores the lookup table with numerical indices, so we still have to map the words to indices before entering the class\n",
    "        #   or tokenization in other means\n",
    "        self.c = nn.Embedding(\n",
    "            num_embeddings=v + 1, embedding_dim=m, padding_idx=self.padding_token_idx\n",
    "        )\n",
    "\n",
    "        #   This way, the linear/Activation layer will take flattened embeddings and produce v outputs\n",
    "        #   I did it this way as per the paper which says that x (output of the embeddings) would be just the concatenation of all feature vectors\n",
    "        #   But you can preserve the dimensionality of the of having each embedding vector seperately even with Linear layers\n",
    "        #   by making the in_features=embedding dimensions\n",
    "        self.linear1 = nn.Linear(in_features=m * (n - 1), out_features=h)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        #   The output layer, linear to create a logits and softmax activation to normalize them to probabilities\n",
    "        self.linear2 = nn.Linear(\n",
    "            in_features=h, out_features=v + 1\n",
    "        )  #   + 1 for the padding token\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.c(x)\n",
    "\n",
    "        linear_activation = self.linear1(embeddings)\n",
    "        non_linear_activation = self.tanh(linear_activation)\n",
    "\n",
    "        logits = self.linear2(non_linear_activation)\n",
    "        probabilities = self.softmax(logits)\n",
    "\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865821e6",
   "metadata": {},
   "source": [
    "# Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4b9d40",
   "metadata": {},
   "source": [
    "I will try to stay close to the paper as much as I can, so we will use brown corpus from nltk, which should be the same they use in the paper, or at least **similar**.  \n",
    "I don't want to drift off so we can compere the results we get to the ones in the paper, but any variations that I do from the one in the paper would probably mean different numbers which means I could've either drifted too much or implemented something incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2251b8cd",
   "metadata": {},
   "source": [
    "## Understand the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "6a6705cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/shadyali/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<CategorizedTaggedCorpusReader in '/Users/shadyali/nltk_data/corpora/brown'>"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "fe290c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "1915fdf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "7cfd86bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56057"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(brown.words()))  # unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "72200864",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_words = [word for word in set(brown.words()) if not word.isalnum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "6c3d5f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " 'sea-food',\n",
       " 'husband-wife',\n",
       " 'herd-owner',\n",
       " \"Skolovsky's\",\n",
       " '7.6%',\n",
       " \"Vonnegut's\",\n",
       " 'working-class',\n",
       " '32-degrees-F',\n",
       " '400-401',\n",
       " 'south-central',\n",
       " 'pistol-packing',\n",
       " 'merry-go-round',\n",
       " 'wine-',\n",
       " \"'55\"]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_index = non_words.index(\".\")\n",
    "non_words[dot_index : dot_index + 15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0356259",
   "metadata": {},
   "source": [
    "So we have about $56k$ unique words, which includes punctuations, words mixed with punctuations, alphanumerals, etc.  \n",
    "The paper replaced all words with frequency $< 3$ with a special token of some kind. And I'm not sure if I understand what's exactly done correctly so I won't do that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9e1508",
   "metadata": {},
   "source": [
    "Let's do some regex for now to fix seperate things like *'herd-owner'*, etc. and then tokenize directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "8f5e5d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\",\n",
       " \"The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\",\n",
       " \"The September-October term jury had been charged by Fulton Superior Court Judge Durwood Pye to investigate reports of possible `` irregularities '' in the hard-fought primary which was won by Mayor-nominate Ivan Allen Jr. .\"]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    \" \".join(sent_words) for sent_words in brown.sents()\n",
    "]  #   Join each sentence to be one string per sentence\n",
    "sentences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8eec41",
   "metadata": {},
   "source": [
    "As you can notice, using join with \" \" puts a space in several places we don't want.  \n",
    "So we'll use regex to solve this. **I ended up using chatGPT to get almost all patterns :(**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "8adca5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place.\", \"The jury further said in term-end presentments that the City Executive Committee, which had over-all charge of the election, `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted.\", \"The September-October term jury had been charged by Fulton Superior Court Judge Durwood Pye to investigate reports of possible `` irregularities '' in the hard-fought primary which was won by Mayor-nominate Ivan Allen Jr..\"]\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "#   Fix the space at the sentences end before the fullstop\n",
    "#   And also the space before commas.\n",
    "\n",
    "punctuation_pattern = re.compile(r\"[ ](?:,)|[ ](?:\\.$)\")\n",
    "sentences = [\n",
    "    re.sub(\n",
    "        punctuation_pattern,\n",
    "        lambda match: match.group().replace(\" \", \"\"),\n",
    "        sentence,\n",
    "    )\n",
    "    for sentence in sentences\n",
    "]\n",
    "print(sentences[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "89d1fcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Fulton County Grand Jury said Friday an investigation of Atlanta\\'s recent primary election produced \"no evidence\" that any irregularities took place.', 'The jury further said in term-end presentments that the City Executive Committee, which had over-all charge of the election, \"deserves the praise and thanks of the City of Atlanta\" for the manner in which the election was conducted.', 'The September-October term jury had been charged by Fulton Superior Court Judge Durwood Pye to investigate reports of possible \"irregularities\" in the hard-fought primary which was won by Mayor-nominate Ivan Allen Jr..']\n"
     ]
    }
   ],
   "source": [
    "#   Lets fix quotes\n",
    "quotes_pattern = re.compile(r\"(?=``).*?(?<='')\")\n",
    "\n",
    "sentences = [\n",
    "    re.sub(\n",
    "        quotes_pattern,\n",
    "        lambda match: match.group().replace(\"`` \", '\"').replace(\" ''\", '\"'),\n",
    "        sentence,\n",
    "    )\n",
    "    for sentence in sentences\n",
    "]\n",
    "print(sentences[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "0126e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = \" \".join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "12b65f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"torney. Hartsfield has been mayor of Atlanta, with exception of one brief interlude, since 1937. His political career goes back to his election to city council in 1923. The mayor's present term of office expires Jan. 1. He will be succeeded by Ivan Allen Jr., who became a candidate in the Sept. 13 primary after Mayor Hartsfield announced that he would not run for reelection. Georgia Republicans are getting strong encouragement to enter a candidate in the 1962 governor's race, a top official said\""
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text[5000:5500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6290833",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e216f26",
   "metadata": {},
   "source": [
    "I will directly use one step of regex used in [gpt-2's tokenizer](https://github.com/openai/gpt-2/blob/master/src/encoder.py), not the whole tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "74883b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "\n",
    "#   Encoding function which splits a sequence (text) into a list\n",
    "def _encode(sequence: str):\n",
    "    pattern = re.compile(\n",
    "        r\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\",\n",
    "        flags=re.IGNORECASE,\n",
    "    )\n",
    "    return re.findall(pattern=pattern, string=sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4e50a8",
   "metadata": {},
   "source": [
    "You can understand what this regex pattern does from [here](https://regex101.com/r/L82kB5/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "a7336063",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(_encode(full_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "6c6baf4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52908"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "7d18e1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Dunes',\n",
       " ' Scrivener',\n",
       " 'Tara',\n",
       " ' Product',\n",
       " ' jure',\n",
       " ' Knightfall',\n",
       " ' Towne',\n",
       " ' recitals',\n",
       " ' dentists',\n",
       " ' paymaster',\n",
       " ' timbre',\n",
       " ' endeavoring',\n",
       " ' brazenly',\n",
       " ' bout',\n",
       " ' froth',\n",
       " ' unities',\n",
       " ' hollows',\n",
       " ' route',\n",
       " ' dead',\n",
       " ' detractors']"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [word for word in vocab]\n",
    "words[20000:20020]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74c7bfd",
   "metadata": {},
   "source": [
    "I feel like this would be good enough to just illustrate things, so I won't try to compress the vocabulary further"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46990c26",
   "metadata": {},
   "source": [
    "We want to add a token for padding, we would need padding when the sequence we have is smaller than the context window. Or if a word is out of the vocab (we haven't encountered before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "9fb600a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_token = {word: token for token, word in enumerate(vocab)}\n",
    "\n",
    "#   Add the special padding token\n",
    "word_to_token[\"\"] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "bfbefcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert max(word_to_token.values()) == len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "f0a79a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' directrices', 0),\n",
       " (' Casino', 1),\n",
       " (' Albany', 2),\n",
       " (' triphenylarsine', 3),\n",
       " (' barnstormer', 4)]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_to_token.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "d4416bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Tokenization function\n",
    "def _tokenize(words: list[str]) -> list[int]:\n",
    "    tokens = [word_to_token.get(word, len(vocab)) for word in words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bebbbf1",
   "metadata": {},
   "source": [
    "Let's test encode and tokenize on the full text now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "bb2bacc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text_tokens = _tokenize(_encode(full_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "b3948238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " ' Fulton',\n",
       " ' County',\n",
       " ' Grand',\n",
       " ' Jury',\n",
       " ' said',\n",
       " ' Friday',\n",
       " ' an',\n",
       " ' investigation',\n",
       " ' of',\n",
       " ' Atlanta',\n",
       " \"'s\",\n",
       " ' recent',\n",
       " ' primary',\n",
       " ' election']"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_encode(full_text)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "3074b276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35378,\n",
       " 10297,\n",
       " 5499,\n",
       " 15230,\n",
       " 24146,\n",
       " 48460,\n",
       " 46026,\n",
       " 13888,\n",
       " 20731,\n",
       " 45927,\n",
       " 19325,\n",
       " 4364,\n",
       " 50112,\n",
       " 29036,\n",
       " 28106]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text_tokens[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "4d015064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35378"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_token[\"The\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "50ebe2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"'\", 'am', ' embedding', ' things', '.']"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_encode(\"I'am embedding things.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "c4b439e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32874, 12369, 37549, 52908, 33490, 9421]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_tokenize(_encode(\"I'am embedding things.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8b0fcf",
   "metadata": {},
   "source": [
    "So there you have it! We can tokenize now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1e5e43",
   "metadata": {},
   "source": [
    "Lets structure our data for training now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bcee3a",
   "metadata": {},
   "source": [
    "## Structuring Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fca7e1f",
   "metadata": {},
   "source": [
    "We will do batch training, so we want to divide our corpus into batches of sequences.  \n",
    "We will make a class for the dataset where it will create a list of a sliding window of $n-1$ words, and the $n^{th}$ is the label.  \n",
    "The class the handle tokenization internally and will provide functions to encode and decode words to tokens and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "9a18e20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class Corpus(Dataset):\n",
    "    def __init__(self, full_text: str, n: int):\n",
    "        super().__init__()\n",
    "        self.context_window: int = n\n",
    "\n",
    "        self.vocab = set(self._encode(full_text))\n",
    "        self.vocab_length: int = len(vocab)\n",
    "\n",
    "        self.word_to_token: dict[str, int] = {\n",
    "            word: token for token, word in enumerate(vocab)\n",
    "        }\n",
    "        self.word_to_token[\"\"] = self.vocab_length\n",
    "\n",
    "        self.token_to_word: dict[int, str] = {\n",
    "            token: word for token, word in enumerate(vocab)\n",
    "        }\n",
    "        self.token_to_word[self.vocab_length] = \"\"\n",
    "\n",
    "        self.full_text_tokenized: list[int] = self._tokenize(self._encode(full_text))\n",
    "        self.define_windows()\n",
    "\n",
    "    # I will reuse the functions and logic from previous cells\n",
    "    def _encode(self, text: str) -> list[str]:\n",
    "        pattern: re.Pattern[str] = re.compile(\n",
    "            r\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\",\n",
    "            flags=re.IGNORECASE,\n",
    "        )\n",
    "        return re.findall(pattern=pattern, string=text)\n",
    "\n",
    "    def _tokenize(self, words: list[str]) -> list[int]:\n",
    "        tokens: list[int] = [\n",
    "            self.word_to_token.get(word, self.vocab_length) for word in words\n",
    "        ]\n",
    "        return tokens\n",
    "\n",
    "    def tokenize_text(cls, text: str) -> torch.Tensor:\n",
    "        return torch.tensor(cls._tokenize(cls._encode(text)))\n",
    "\n",
    "    def decode(self, tokens: torch.Tensor) -> list[str]:\n",
    "        tokens = tokens.tolist() if tokens.shape else [tokens.tolist()] # A scaler value will output the value without list, raising an error below\n",
    "        \n",
    "        words = [self.token_to_word[token] for token in tokens]\n",
    "        return words\n",
    "\n",
    "    def define_windows(self) -> None:\n",
    "        self.sequences = []\n",
    "        self.next_token = []\n",
    "\n",
    "        text_length = len(self.full_text_tokenized)\n",
    "        for index in range(text_length):\n",
    "            if index + self.context_window - 1 < text_length:\n",
    "                self.sequences.append(\n",
    "                    self.full_text_tokenized[index : index + self.context_window - 1]\n",
    "                )\n",
    "                self.next_token.append(\n",
    "                    self.full_text_tokenized[index + self.context_window - 1]\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, window):\n",
    "        return torch.tensor(self.sequences[window]), torch.tensor(self.next_token[window])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "4fd1cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Corpus(full_text=full_text, n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "9fa60910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([' of', ' Atlanta', '\"'], [' for'])"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.decode(dataset[60][0]), dataset.decode(dataset[60][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bd55ce",
   "metadata": {},
   "source": [
    "We can now use the DataLoader class to get an iterator to produce batches of training data for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "5a1f04a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_iterator = iter(DataLoader(\n",
    "    dataset=dataset, batch_size=50, shuffle=True\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "3f7cad6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([50, 3])\n",
      "Labels batch shape: torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "input, label = next(data_iterator)\n",
    "print(f\"Feature batch shape: {input.shape}\")\n",
    "print(f\"Labels batch shape: {label.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b40c65a",
   "metadata": {},
   "source": [
    "The loader is working!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f069b508",
   "metadata": {},
   "source": [
    "I think we are ready now to split our data, ***AND TRAIN OUR MODEL***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdf012b",
   "metadata": {},
   "source": [
    "# MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34258e37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
