# Word-Embeddings-from-Scratch
Over the past couple of months I've been working on multiple projects with agents and RAG, which made me use and interact with word embedding models alot, I also was trying to understand recommender systems and graph neural networks. So...  
I wanted to learn about word embeddings from like, the beginning, to understand it and the intuitions and mathematical concepts behind it. I specifically aimed to read Word2Vec paper and implement it from scratch, but then I gone a step back to understand earlier work for neural language models, and then the language modelling in general as statistical modelling.  

So the first step I took was learning n-grams language models from ***Speech and Language Processing: An Introduction to Natural Language Processing Computational Linguistics, and Speech Recognition with Language Models*** *Third Edition draft* from *Daniel Jurafsky*, and *James H. Martin*.

I made a small notebook that explained the main concepts I learned and a simple implementation of bigram model on a Twitter Hate Speech dataset (because the generated sentences will be funny).

Next step is ***A neural probabilistic language model*** paper from (*Bengio, Y., et al, 2003*).
